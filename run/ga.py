import os
from pprint import pprint 
import subprocess
import numpy as np
import sys 
import resource
from util import make_shuffle
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

run_id = os.popen("date +%s | sha256sum | base64 | head -c 8").read()

# setting up the basic paths
# read the env for the base dir 
base_dir = os.environ.get("PSIM_BASE_DIR")
print("base_dir:", base_dir)
build_path = base_dir + "/build"
run_path = base_dir + "/run/"
base_executable = build_path + "/psim"
executable = build_path + "/psim-" + run_id
input_dir = base_dir + "/input/"
shuffle_path  = input_dir + "/shuffle/shuffle-{}.txt".format(run_id)
use_gdb = False
worker_count = 30

# build the executable, exit if build fails
os.chdir(build_path)
# run the make -j command, get the exit code
exit_code = os.system("make -j")
if exit_code != 0:
    print("make failed, exiting")
    sys.exit(1)
os.chdir(run_path)
os.system("cp {} {}".format(base_executable, executable))

make_shuffle(128, shuffle_path)


# get the parameters from the command line
params = {}
for i, arg in enumerate(sys.argv):
    if i == 0:
        continue
    p = arg.split("=")
    key = p[0][2:]
    if len(p) == 1:
        val = True
    else:
        val = p[1]
        if val == "true":
            val = True
        if val == "false":
            val = False
    params[key] = val
    
if "gdb" in params:
    use_gdb = True
    del params["gdb"]

options = {
    "protocol-file-dir": base_dir + "/input/128search-dpstart-2",
    "protocol-file-name": "candle128-simtime.txt",
    # "protocol-file-name": "transformer128-simtime+compute.txt",

    "step-size": 10,
    "core-status-profiling-interval": 100,
    "rep-count": 1, 
    "console-log-level": 4,
    "file-log-level": 3,
    
    "initial-rate": 100,
    "min-rate": 100,
    "priority-allocator": "fairshare", #"priorityqueue", 

    "network-type": "leafspine",    
    "link-bandwidth": 100,
    "ft-server-per-rack": 8,
    "ft-rack-per-pod": 4,
    "ft-agg-per-pod": 4,
    "ft-core-count": 4,
    "ft-pod-count": 4,
    "ft-server-tor-link-capacity-mult": 1,
    "ft-tor-agg-link-capacity-mult": 1,
    "ft-agg-core-link-capacity-mult": 1,
    
    
    "lb-scheme": "readfile",
    "load-metric": "utilization",
    "shuffle-device-map": True,
    "shuffle-map-file": input_dir + "/shuffle/shuffle-map.txt",
}

options.update(params)

def make_cmd(executable, options):
    cmd = executable
    for option in options.items():
        if option[1] is False:
            continue
        elif option[1] is True:
            cmd += " --" + option[0]
        else: 
            cmd += " --" + option[0] + "=" + str(option[1])

    if use_gdb:
        cmd = "gdb -ex run --args " + cmd

    return cmd
    

# subprocess.run(cmd, stdout=sys.stdout, stderr=sys.stderr, shell=True)

# run the command, print the output as it's generated by the subprocess
# maintain the color coding of the output

core_count = options["ft-core-count"]

memory_limit_kb = 10 * 1e9
memory_limit_kb = int(memory_limit_kb)
resource.setrlimit(resource.RLIMIT_AS, (memory_limit_kb, memory_limit_kb))


def decisions_file_to_map(file_path):
    # line format: bluh... flow {} core {}
    
    decisions = {}
    
    with open(file_path, "r") as f:
        for line in f:
            if "flow" not in line:
                continue
            flow = int(line.split("flow ")[1].split(" ")[0])
            core = int(line.split("core ")[1].split(" ")[0])
            decisions[flow] = core
            
    return decisions


def decisions_map_to_file(decisions, file_path):
    with open(file_path, "w") as f:
        for flow, core in decisions.items():
            f.write("flow {} core {}\n".format(flow, core))


def genetic_permute(decisions_map, num_permutations=1):
    new_decisions = decisions_map.copy()
        
    for i in range(num_permutations):
        flow = np.random.choice(list(decisions_map.keys()))
        core = np.random.randint(0, core_count)    
        new_decisions[flow] = core
    
    return new_decisions


# keep the same keys, but randomize the values, for all the keys
def genetic_random(decisions_map):
    new_decisions = decisions_map.copy()
    
    for flow in decisions_map.keys():
        core_count = options["ft-core-count"]
        core = np.random.randint(0, core_count)
        new_decisions[flow] = core
        
    return new_decisions                

def genetic_crossover(decisions_map1, decisions_map2, crossover_method="random"):
    
    if crossover_method == "random":
        new_decisions = decisions_map1.copy()

        for flow in decisions_map1.keys():
            if np.random.random() < 0.5:
                new_decisions[flow] = decisions_map2[flow]
                    
        return new_decisions
    
    if crossover_method == "combine":
        new_decisions = decisions_map1.copy()

        num_keys = len(decisions_map1.keys())
        keys = list(decisions_map1.keys())

        # change the second half of the keys to the second map
        for i in range(num_keys // 2, num_keys):
            new_decisions[keys[i]] = decisions_map2[keys[i]]
            
        return new_decisions
    
    
# population is a map from worker id to decisions file path
def evaluate_population(population): 
    
    # run the jobs in parallel
    jobs = []
    job_options = options.copy()
    
    for i, specimen in population.items():
                
        job_options["worker-id"] = i
        job_options["lb-decisions-file"] = specimen["path"]
        
        cmd = make_cmd(executable, job_options)

        jobs.append(subprocess.Popen(cmd, 
                                     stdout=subprocess.PIPE, 
                                     stderr=subprocess.STDOUT, 
                                     shell=True, 
                                     preexec_fn=os.setsid))
    
    # wait for all the jobs to finish
    for job in jobs:
        job.wait()
    
    # get the psim times from the output
    results = {}
    
    for i, job in enumerate(jobs):
        for line in iter(job.stdout.readline, b''):
            output = line.decode("utf-8")
            if "psim time:" in output:
                psim_time = float(output.split("psim time:")[1])
                results[i] = {
                    "time": psim_time, 
                    "decisions": population[i]["path"], 
                    "method": population[i]["method"]
                }
                break       
    
    return results


initial_decisions = decisions_file_to_map("lb-decisions/init/init.txt")


def make_next_population(i, new_round_dir, sorted_results):
    keep_limit = int(worker_count * 0.2)
    permute_limit = int(worker_count * 0.5)
    crossover_limit = int(worker_count * 0.9)
    
    new_path = "{}/decisions-{}.txt".format(new_round_dir, i)
    method = "" 
    
    if i < keep_limit:
        method = "\033[92m" + "topscore" + "\033[0m"
        
        worker_decisions_path = sorted_results[i][1]["decisions"]
        worker_decisions = decisions_file_to_map(worker_decisions_path)
        decisions_map_to_file(worker_decisions, new_path)
        
    elif i < permute_limit:
        method = "\033[94m" + "permute" + "\033[0m"
        
        permutation_source = np.random.randint(0, keep_limit)
        permutation_count = np.random.randint(20, 100)
        
        worker_decisions_path = sorted_results[permutation_source][1]["decisions"]
        worker_decisions = decisions_file_to_map(worker_decisions_path)
        new_worker_decisions = genetic_permute(worker_decisions, permutation_count)
        decisions_map_to_file(new_worker_decisions, new_path)
        
        
        
    elif i < crossover_limit:
        method = "\033[93m" + "crossover" + "\033[0m"
        
        crossover_source1 = np.random.randint(0, keep_limit)
        crossover_source2 = np.random.randint(0, worker_count)
        crossover_method = np.random.choice(["random", "combine"])
        
        worker_decisions_path1 = sorted_results[crossover_source1][1]["decisions"]
        worker_decisions_path2 = sorted_results[crossover_source2][1]["decisions"]
        
        worker_decisions1 = decisions_file_to_map(worker_decisions_path1)
        worker_decisions2 = decisions_file_to_map(worker_decisions_path2)
        
        new_worker_decisions = genetic_crossover(worker_decisions1, worker_decisions2, crossover_method)
        decisions_map_to_file(new_worker_decisions, new_path)
        
    else:
        method = "\033[91m" + "random" + "\033[0m"
        
        worker_decisions = genetic_random(initial_decisions)
        decisions_map_to_file(worker_decisions, new_path)

    return {"path": new_path, 
            "method": method} 
    
def genetic_algorithm(initial_decisions):
    num_rounds = 20
    
    os.system("mkdir -p lb-decisions/round_0")
    
    population = {}
    
    baseline_count = 9 
    for i in range(worker_count):
        worker_decisions_path = "lb-decisions/round_0/decisions-{}.txt".format(i)
        
        if i < baseline_count:
            baseline_path = "lb-decisions/baselines/{}.txt".format(i)
            baseline_decisions = decisions_file_to_map(baseline_path)
            decisions_map_to_file(baseline_decisions, worker_decisions_path)

            population[i] = {"path" : worker_decisions_path, 
                             "method": "\033[92m" + "baseline" + "\033[0m"}
            
        else: 
            worker_decisions = genetic_random(initial_decisions)
            decisions_map_to_file(worker_decisions, worker_decisions_path)
            
            population[i] = {"path" : worker_decisions_path, 
                             "method": "\033[91m" + "random" + "\033[0m"}

     
    for round in range(num_rounds):
        results = evaluate_population(population)
        sorted_results = sorted(results.items(), key=lambda kv: kv[1]["time"])
        for i, result in enumerate(sorted_results):
            print(i, result[1]["time"], result[1]["method"])
            
        # prepare the next round
        new_round_dir = "lb-decisions/round_{}".format(round + 1)
        os.system("mkdir -p {}".format(new_round_dir))  


        # def worker(i):
        #     print("worker {} started".format(i))
        #     return i, make_next_population(i, new_round_dir, sorted_results)

        # next_population = {}
        # with ThreadPoolExecutor() as executor:
        #     futures = {executor.submit(worker, i) for i in range(worker_count)}
        #     for future in as_completed(futures):
        #         i, result = future.result()
        #         print("worker {} finished".format(i))
        #         next_population[i] = result
  
        print("making next population: ", end="")
        next_population = {}
        for i in range(worker_count):
            next_population[i] = make_next_population(i, new_round_dir, sorted_results)
                  
        population = next_population
        os.system("rm -rf lb-decisions/round_{}".format(round))
        print("round {} finished.\n\n\n\n".format(round))


genetic_algorithm(initial_decisions)

os.system("rm {}".format(executable))